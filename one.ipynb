{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Data Collection from user"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Collect drowsy state video"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Disclaimer: Please act drowsy for the next 10 seconds.\n",
      "starting in 3\n",
      "starting in 2\n",
      "starting in 1\n",
      "Recording video for 10 seconds...\n"
     ]
    }
   ],
   "source": [
    "# Disclaimer: Inform the user to act drowsy\n",
    "print(\"Disclaimer: Please act drowsy for the next 10 seconds.\")\n",
    "#setting the disclaimer to 10 seconnds to allow the user to act drowsy\n",
    "\n",
    "import cv2\n",
    "import os\n",
    "import time\n",
    "\n",
    "time.sleep(1)\n",
    "print(\"starting in 3\")\n",
    "time.sleep(1)\n",
    "print(\"starting in 2\")\n",
    "time.sleep(1)\n",
    "print(\"starting in 1\")\n",
    "\n",
    "def capture_video_and_save_frames(duration, output_folder):\n",
    "    \"\"\"\n",
    "    Capture video for a given duration and save frames as JPEG images.\n",
    "    \n",
    "    Parameters:\n",
    "        duration (int): Duration to capture video in seconds.\n",
    "        output_folder (str): Path to save the captured frames.\n",
    "    \"\"\"\n",
    "    # Create the output directory if it doesn't exist\n",
    "    if not os.path.exists(output_folder):\n",
    "        os.makedirs(output_folder)\n",
    "\n",
    "    # Open the default camera\n",
    "    cap = cv2.VideoCapture(0)\n",
    "\n",
    "    # Check if the camera opened successfully\n",
    "    if not cap.isOpened():\n",
    "        print(\"Error: Cannot open webcam.\")\n",
    "        return\n",
    "\n",
    "    print(f\"Recording video for {duration} seconds...\")\n",
    "    start_time = time.time()\n",
    "    frame_count = 0\n",
    "\n",
    "    while True:\n",
    "        # Capture frame-by-frame\n",
    "        ret, frame = cap.read()\n",
    "\n",
    "        # If frame is read correctly, ret is True\n",
    "        if not ret:\n",
    "            print(\"Error: Can't receive frame (stream end?). Exiting...\")\n",
    "            break\n",
    "\n",
    "        # Save frame as JPEG file\n",
    "        frame_filename = os.path.join(output_folder, f'frame_{frame_count:04d}.jpg')\n",
    "        cv2.imwrite(frame_filename, frame)\n",
    "        frame_count += 1\n",
    "\n",
    "        # Display the resulting frame\n",
    "        cv2.imshow('Frame', frame)\n",
    "\n",
    "        # Stop capturing after the specified duration\n",
    "        if time.time() - start_time > duration:\n",
    "            break\n",
    "\n",
    "        # Break the loop on 'q' key press\n",
    "        if cv2.waitKey(1) & 0xFF == ord('q'):\n",
    "            break\n",
    "\n",
    "    # Release the capture and close windows\n",
    "    cap.release()\n",
    "    cv2.destroyAllWindows()\n",
    "\n",
    "# Record video of drowsy behavior (10 seconds)\n",
    "capture_video_and_save_frames(10, 'drowsy_frames')\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Collect Non Drowsy Sate Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Please wait for 5 seconds.\n",
      "Please act normal for the next 10 seconds.\n",
      "Recording video for 10 seconds...\n"
     ]
    }
   ],
   "source": [
    "# Wait for 5 seconds\n",
    "print(\"Please wait for 5 seconds.\")\n",
    "time.sleep(5)\n",
    "def capture_video_and_save_frames1(duration, output_folder):\n",
    "    \"\"\"\n",
    "    Capture video for a given duration and save frames as JPEG images.\n",
    "    \n",
    "    Parameters:\n",
    "        duration (int): Duration to capture video in seconds.\n",
    "        output_folder (str): Path to save the captured frames.\n",
    "    \"\"\"\n",
    "    # Create the output directory if it doesn't exist\n",
    "    if not os.path.exists(output_folder):\n",
    "        os.makedirs(output_folder)\n",
    "\n",
    "    # Open the default camera\n",
    "    cap = cv2.VideoCapture(0)\n",
    "\n",
    "    # Check if the camera opened successfully\n",
    "    if not cap.isOpened():\n",
    "        print(\"Error: Cannot open webcam.\")\n",
    "        return\n",
    "\n",
    "    print(f\"Recording video for {duration} seconds...\")\n",
    "    start_time = time.time()\n",
    "    frame_count = 0\n",
    "\n",
    "    while True:\n",
    "        # Capture frame-by-frame\n",
    "        ret, frame = cap.read()\n",
    "\n",
    "        # If frame is read correctly, ret is True\n",
    "        if not ret:\n",
    "            print(\"Error: Can't receive frame (stream end?). Exiting...\")\n",
    "            break\n",
    "\n",
    "        # Save frame as JPEG file\n",
    "        frame_filename = os.path.join(output_folder, f'frame_{frame_count:04d}.jpg')\n",
    "        cv2.imwrite(frame_filename, frame)\n",
    "        frame_count += 1\n",
    "\n",
    "        # Display the resulting frame\n",
    "        cv2.imshow('Frame', frame)\n",
    "\n",
    "        # Stop capturing after the specified duration\n",
    "        if time.time() - start_time > duration:\n",
    "            break\n",
    "\n",
    "        # Break the loop on 'q' key press\n",
    "        if cv2.waitKey(1) & 0xFF == ord('q'):\n",
    "            break\n",
    "\n",
    "    # Release the capture and close windows\n",
    "    cap.release()\n",
    "    cv2.destroyAllWindows()\n",
    "\n",
    "# Disclaimer: Inform the user to act normal\n",
    "print(\"Please act normal for the next 10 seconds.\")\n",
    "\n",
    "# Record video of normal behavior (10 seconds)\n",
    "capture_video_and_save_frames1(10, 'normal_frames')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Importing necessary libraries for facial feature detection and loading the facial detection"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# la\n",
    "from scipy.spatial import distance\n",
    "import dlib\n",
    "\n",
    "# Function to calculate Eye Aspect Ratio (EAR)\n",
    "def calculate_ear(eye):\n",
    "    A = distance.euclidean(eye[1], eye[5])\n",
    "    B = distance.euclidean(eye[2], eye[4])\n",
    "    C = distance.euclidean(eye[0], eye[3])\n",
    "    ear = (A + B) / (2.0 * C)\n",
    "    return ear\n",
    "\n",
    "# Function to calculate Mouth Aspect Ratio (MAR)\n",
    "def calculate_mar(mouth):\n",
    "    A = distance.euclidean(mouth[3], mouth[9])\n",
    "    B = distance.euclidean(mouth[2], mouth[10])\n",
    "    C = distance.euclidean(mouth[4], mouth[8])\n",
    "    D = distance.euclidean(mouth[0], mouth[6])\n",
    "    mar = (A + B + C) / (3.0 * D)\n",
    "    return mar\n",
    "\n",
    "# Load the facial landmark detector\n",
    "detector = dlib.get_frontal_face_detector()\n",
    "predictor = dlib.shape_predictor(\"shape_predictor_68_face_landmarks.dat\")  # Download required\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### extracting the features \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Feature extraction code remains similar, reuse the EAR and MAR calculation functions\n",
    "# Extract features from 'drowsy_frames' and 'normal_frames' folders.\n",
    "# Save features as a .csv file for training later.\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import glob\n",
    "\n",
    "def extract_features_from_frames(input_folder, label, output_csv):\n",
    "    \"\"\"\n",
    "    Extract EAR, MAR, and other features from frames and save to CSV.\n",
    "    \"\"\"\n",
    "    data = []\n",
    "    files = sorted(glob.glob(os.path.join(input_folder, \"*.jpg\")))\n",
    "\n",
    "    for file in files:\n",
    "        frame = cv2.imread(file)\n",
    "        gray = cv2.cvtColor(frame, cv2.COLOR_BGR2GRAY)\n",
    "        faces = detector(gray)\n",
    "\n",
    "        for face in faces:\n",
    "            landmarks = predictor(gray, face)\n",
    "            left_eye = np.array([(landmarks.part(i).x, landmarks.part(i).y) for i in range(36, 42)])\n",
    "            right_eye = np.array([(landmarks.part(i).x, landmarks.part(i).y) for i in range(42, 48)])\n",
    "            mouth = np.array([(landmarks.part(i).x, landmarks.part(i).y) for i in range(48, 68)])\n",
    "\n",
    "            ear = (calculate_ear(left_eye) + calculate_ear(right_eye)) / 2.0\n",
    "            mar = calculate_mar(mouth)\n",
    "            data.append([ear, mar, label])\n",
    "\n",
    "    # Save data to CSV\n",
    "    df = pd.DataFrame(data, columns=['EAR', 'MAR', 'Label'])\n",
    "    df.to_csv(output_csv, index=False)\n",
    "\n",
    "# Extract features for both classes\n",
    "extract_features_from_frames('drowsy_frames', 1, 'drowsy_features.csv')\n",
    "extract_features_from_frames('normal_frames', 0, 'normal_features.csv')\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Training the model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/50\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Library/Frameworks/Python.framework/Versions/3.11/lib/python3.11/site-packages/keras/src/layers/core/dense.py:87: UserWarning: Do not pass an `input_shape`/`input_dim` argument to a layer. When using Sequential models, prefer using an `Input(shape)` object as the first layer in the model instead.\n",
      "  super().__init__(activity_regularizer=activity_regularizer, **kwargs)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[1m30/30\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 3ms/step - accuracy: 0.7251 - loss: 0.6428 - val_accuracy: 0.7373 - val_loss: 0.6249\n",
      "Epoch 2/50\n",
      "\u001b[1m30/30\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 1ms/step - accuracy: 0.7790 - loss: 0.5912 - val_accuracy: 0.7458 - val_loss: 0.5958\n",
      "Epoch 3/50\n",
      "\u001b[1m30/30\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 1ms/step - accuracy: 0.7946 - loss: 0.5668 - val_accuracy: 0.7458 - val_loss: 0.5712\n",
      "Epoch 4/50\n",
      "\u001b[1m30/30\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 885us/step - accuracy: 0.7879 - loss: 0.5392 - val_accuracy: 0.7627 - val_loss: 0.5443\n",
      "Epoch 5/50\n",
      "\u001b[1m30/30\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 859us/step - accuracy: 0.7996 - loss: 0.5126 - val_accuracy: 0.7712 - val_loss: 0.5174\n",
      "Epoch 6/50\n",
      "\u001b[1m30/30\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 870us/step - accuracy: 0.7961 - loss: 0.4782 - val_accuracy: 0.7797 - val_loss: 0.4958\n",
      "Epoch 7/50\n",
      "\u001b[1m30/30\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 879us/step - accuracy: 0.8379 - loss: 0.4365 - val_accuracy: 0.7881 - val_loss: 0.4785\n",
      "Epoch 8/50\n",
      "\u001b[1m30/30\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 873us/step - accuracy: 0.8274 - loss: 0.4258 - val_accuracy: 0.7966 - val_loss: 0.4628\n",
      "Epoch 9/50\n",
      "\u001b[1m30/30\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 824us/step - accuracy: 0.8472 - loss: 0.3715 - val_accuracy: 0.8051 - val_loss: 0.4512\n",
      "Epoch 10/50\n",
      "\u001b[1m30/30\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 806us/step - accuracy: 0.8113 - loss: 0.4285 - val_accuracy: 0.8136 - val_loss: 0.4388\n",
      "Epoch 11/50\n",
      "\u001b[1m30/30\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 858us/step - accuracy: 0.7902 - loss: 0.4058 - val_accuracy: 0.8136 - val_loss: 0.4295\n",
      "Epoch 12/50\n",
      "\u001b[1m30/30\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 858us/step - accuracy: 0.8355 - loss: 0.3742 - val_accuracy: 0.8136 - val_loss: 0.4186\n",
      "Epoch 13/50\n",
      "\u001b[1m30/30\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 944us/step - accuracy: 0.8454 - loss: 0.3596 - val_accuracy: 0.8136 - val_loss: 0.4094\n",
      "Epoch 14/50\n",
      "\u001b[1m30/30\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 847us/step - accuracy: 0.8356 - loss: 0.3421 - val_accuracy: 0.8220 - val_loss: 0.3994\n",
      "Epoch 15/50\n",
      "\u001b[1m30/30\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 841us/step - accuracy: 0.8537 - loss: 0.3298 - val_accuracy: 0.8220 - val_loss: 0.3904\n",
      "Epoch 16/50\n",
      "\u001b[1m30/30\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 856us/step - accuracy: 0.8425 - loss: 0.3481 - val_accuracy: 0.8220 - val_loss: 0.3825\n",
      "Epoch 17/50\n",
      "\u001b[1m30/30\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 850us/step - accuracy: 0.8221 - loss: 0.3627 - val_accuracy: 0.8220 - val_loss: 0.3733\n",
      "Epoch 18/50\n",
      "\u001b[1m30/30\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 820us/step - accuracy: 0.8422 - loss: 0.3386 - val_accuracy: 0.8220 - val_loss: 0.3658\n",
      "Epoch 19/50\n",
      "\u001b[1m30/30\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 837us/step - accuracy: 0.8696 - loss: 0.3136 - val_accuracy: 0.8220 - val_loss: 0.3600\n",
      "Epoch 20/50\n",
      "\u001b[1m30/30\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 866us/step - accuracy: 0.8507 - loss: 0.3206 - val_accuracy: 0.8220 - val_loss: 0.3543\n",
      "Epoch 21/50\n",
      "\u001b[1m30/30\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 856us/step - accuracy: 0.8273 - loss: 0.3634 - val_accuracy: 0.8220 - val_loss: 0.3477\n",
      "Epoch 22/50\n",
      "\u001b[1m30/30\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 852us/step - accuracy: 0.8488 - loss: 0.3245 - val_accuracy: 0.8305 - val_loss: 0.3428\n",
      "Epoch 23/50\n",
      "\u001b[1m30/30\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 821us/step - accuracy: 0.8590 - loss: 0.3075 - val_accuracy: 0.8305 - val_loss: 0.3394\n",
      "Epoch 24/50\n",
      "\u001b[1m30/30\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 820us/step - accuracy: 0.8427 - loss: 0.3095 - val_accuracy: 0.8305 - val_loss: 0.3328\n",
      "Epoch 25/50\n",
      "\u001b[1m30/30\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 810us/step - accuracy: 0.8337 - loss: 0.3615 - val_accuracy: 0.8305 - val_loss: 0.3274\n",
      "Epoch 26/50\n",
      "\u001b[1m30/30\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 820us/step - accuracy: 0.8732 - loss: 0.2771 - val_accuracy: 0.8305 - val_loss: 0.3224\n",
      "Epoch 27/50\n",
      "\u001b[1m30/30\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 859us/step - accuracy: 0.8423 - loss: 0.2915 - val_accuracy: 0.8390 - val_loss: 0.3176\n",
      "Epoch 28/50\n",
      "\u001b[1m30/30\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 864us/step - accuracy: 0.8679 - loss: 0.3073 - val_accuracy: 0.8390 - val_loss: 0.3133\n",
      "Epoch 29/50\n",
      "\u001b[1m30/30\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 1ms/step - accuracy: 0.8508 - loss: 0.3202 - val_accuracy: 0.8390 - val_loss: 0.3086\n",
      "Epoch 30/50\n",
      "\u001b[1m30/30\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 861us/step - accuracy: 0.8776 - loss: 0.2632 - val_accuracy: 0.8390 - val_loss: 0.3051\n",
      "Epoch 31/50\n",
      "\u001b[1m30/30\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 863us/step - accuracy: 0.8577 - loss: 0.2694 - val_accuracy: 0.8390 - val_loss: 0.3012\n",
      "Epoch 32/50\n",
      "\u001b[1m30/30\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 818us/step - accuracy: 0.8265 - loss: 0.3151 - val_accuracy: 0.8475 - val_loss: 0.2948\n",
      "Epoch 33/50\n",
      "\u001b[1m30/30\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 826us/step - accuracy: 0.8649 - loss: 0.2700 - val_accuracy: 0.8475 - val_loss: 0.2924\n",
      "Epoch 34/50\n",
      "\u001b[1m30/30\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 836us/step - accuracy: 0.8769 - loss: 0.2601 - val_accuracy: 0.8475 - val_loss: 0.2895\n",
      "Epoch 35/50\n",
      "\u001b[1m30/30\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 823us/step - accuracy: 0.8711 - loss: 0.2788 - val_accuracy: 0.8475 - val_loss: 0.2859\n",
      "Epoch 36/50\n",
      "\u001b[1m30/30\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 847us/step - accuracy: 0.8735 - loss: 0.2898 - val_accuracy: 0.8475 - val_loss: 0.2830\n",
      "Epoch 37/50\n",
      "\u001b[1m30/30\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 873us/step - accuracy: 0.8723 - loss: 0.2691 - val_accuracy: 0.8475 - val_loss: 0.2798\n",
      "Epoch 38/50\n",
      "\u001b[1m30/30\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 816us/step - accuracy: 0.8702 - loss: 0.2731 - val_accuracy: 0.8475 - val_loss: 0.2768\n",
      "Epoch 39/50\n",
      "\u001b[1m30/30\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 836us/step - accuracy: 0.8868 - loss: 0.2618 - val_accuracy: 0.8390 - val_loss: 0.2733\n",
      "Epoch 40/50\n",
      "\u001b[1m30/30\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 792us/step - accuracy: 0.8852 - loss: 0.2387 - val_accuracy: 0.8390 - val_loss: 0.2696\n",
      "Epoch 41/50\n",
      "\u001b[1m30/30\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 799us/step - accuracy: 0.8841 - loss: 0.2623 - val_accuracy: 0.8390 - val_loss: 0.2677\n",
      "Epoch 42/50\n",
      "\u001b[1m30/30\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 781us/step - accuracy: 0.8722 - loss: 0.2576 - val_accuracy: 0.8475 - val_loss: 0.2658\n",
      "Epoch 43/50\n",
      "\u001b[1m30/30\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 779us/step - accuracy: 0.8860 - loss: 0.2582 - val_accuracy: 0.8559 - val_loss: 0.2630\n",
      "Epoch 44/50\n",
      "\u001b[1m30/30\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 846us/step - accuracy: 0.8791 - loss: 0.2855 - val_accuracy: 0.8559 - val_loss: 0.2614\n",
      "Epoch 45/50\n",
      "\u001b[1m30/30\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 854us/step - accuracy: 0.8756 - loss: 0.2601 - val_accuracy: 0.8559 - val_loss: 0.2585\n",
      "Epoch 46/50\n",
      "\u001b[1m30/30\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 861us/step - accuracy: 0.9008 - loss: 0.2595 - val_accuracy: 0.8559 - val_loss: 0.2576\n",
      "Epoch 47/50\n",
      "\u001b[1m30/30\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 841us/step - accuracy: 0.9086 - loss: 0.2225 - val_accuracy: 0.8559 - val_loss: 0.2562\n",
      "Epoch 48/50\n",
      "\u001b[1m30/30\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 750us/step - accuracy: 0.9024 - loss: 0.2133 - val_accuracy: 0.8559 - val_loss: 0.2536\n",
      "Epoch 49/50\n",
      "\u001b[1m30/30\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 810us/step - accuracy: 0.8961 - loss: 0.2548 - val_accuracy: 0.8475 - val_loss: 0.2519\n",
      "Epoch 50/50\n",
      "\u001b[1m30/30\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 788us/step - accuracy: 0.9049 - loss: 0.2357 - val_accuracy: 0.8475 - val_loss: 0.2493\n",
      "\u001b[1m4/4\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 697us/step - accuracy: 0.8577 - loss: 0.2573\n",
      "Test Accuracy: 84.75%\n"
     ]
    }
   ],
   "source": [
    "import tensorflow as tf\n",
    "from tensorflow.keras.models import Sequential\n",
    "from tensorflow.keras.layers import Dense, Dropout\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "\n",
    "# Load data\n",
    "drowsy_data = pd.read_csv('drowsy_features.csv')\n",
    "normal_data = pd.read_csv('normal_features.csv')\n",
    "data = pd.concat([drowsy_data, normal_data])\n",
    "\n",
    "X = data[['EAR', 'MAR']].values\n",
    "y = data['Label'].values\n",
    "\n",
    "# Normalize features\n",
    "scaler = StandardScaler()\n",
    "X = scaler.fit_transform(X)\n",
    "\n",
    "# Split data\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n",
    "\n",
    "# Build a lightweight neural network\n",
    "model = Sequential([\n",
    "    Dense(8, input_shape=(2,), activation='relu'),\n",
    "    Dropout(0.2),\n",
    "    Dense(10, activation='relu'),\n",
    "    Dense(1, activation='sigmoid')  # Binary classification\n",
    "])\n",
    "\n",
    "model.compile(optimizer='adam', loss='binary_crossentropy', metrics=['accuracy'])\n",
    "\n",
    "# Train the model\n",
    "history = model.fit(X_train, y_train, validation_data=(X_test, y_test), epochs=50, batch_size=16)\n",
    "\n",
    "# Evaluate the model\n",
    "test_loss, test_accuracy = model.evaluate(X_test, y_test)\n",
    "print(f\"Test Accuracy: {test_accuracy * 100:.2f}%\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Converting to tflite code and optimising it out"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Assets written to: /var/folders/5d/cxg576h92j906wv9p0xyjc3c0000gn/T/tmp3y2lrokq/assets\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Assets written to: /var/folders/5d/cxg576h92j906wv9p0xyjc3c0000gn/T/tmp3y2lrokq/assets\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Saved artifact at '/var/folders/5d/cxg576h92j906wv9p0xyjc3c0000gn/T/tmp3y2lrokq'. The following endpoints are available:\n",
      "\n",
      "* Endpoint 'serve'\n",
      "  args_0 (POSITIONAL_ONLY): TensorSpec(shape=(None, 2), dtype=tf.float32, name='keras_tensor')\n",
      "Output Type:\n",
      "  TensorSpec(shape=(None, 1), dtype=tf.float32, name=None)\n",
      "Captures:\n",
      "  12146675152: TensorSpec(shape=(), dtype=tf.resource, name=None)\n",
      "  12198575952: TensorSpec(shape=(), dtype=tf.resource, name=None)\n",
      "  12198575568: TensorSpec(shape=(), dtype=tf.resource, name=None)\n",
      "  12198574416: TensorSpec(shape=(), dtype=tf.resource, name=None)\n",
      "  12203147920: TensorSpec(shape=(), dtype=tf.resource, name=None)\n",
      "  12203148496: TensorSpec(shape=(), dtype=tf.resource, name=None)\n",
      "Model saved as drowsiness_model.tflite\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING: All log messages before absl::InitializeLog() is called are written to STDERR\n",
      "W0000 00:00:1731582251.263766   38143 tf_tfl_flatbuffer_helpers.cc:365] Ignored output_format.\n",
      "W0000 00:00:1731582251.263816   38143 tf_tfl_flatbuffer_helpers.cc:368] Ignored drop_control_dependency.\n",
      "2024-11-14 16:34:11.264053: I tensorflow/cc/saved_model/reader.cc:83] Reading SavedModel from: /var/folders/5d/cxg576h92j906wv9p0xyjc3c0000gn/T/tmp3y2lrokq\n",
      "2024-11-14 16:34:11.264399: I tensorflow/cc/saved_model/reader.cc:52] Reading meta graph with tags { serve }\n",
      "2024-11-14 16:34:11.264405: I tensorflow/cc/saved_model/reader.cc:147] Reading SavedModel debug info (if present) from: /var/folders/5d/cxg576h92j906wv9p0xyjc3c0000gn/T/tmp3y2lrokq\n",
      "I0000 00:00:1731582251.267335   38143 mlir_graph_optimization_pass.cc:401] MLIR V1 optimization pass is not enabled\n",
      "2024-11-14 16:34:11.267835: I tensorflow/cc/saved_model/loader.cc:236] Restoring SavedModel bundle.\n",
      "2024-11-14 16:34:11.286802: I tensorflow/cc/saved_model/loader.cc:220] Running initialization op on SavedModel bundle at path: /var/folders/5d/cxg576h92j906wv9p0xyjc3c0000gn/T/tmp3y2lrokq\n",
      "2024-11-14 16:34:11.292591: I tensorflow/cc/saved_model/loader.cc:466] SavedModel load for tags { serve }; Status: success: OK. Took 28540 microseconds.\n",
      "2024-11-14 16:34:11.301310: I tensorflow/compiler/mlir/tensorflow/utils/dump_mlir_util.cc:268] disabling MLIR crash reproducer, set env var `MLIR_CRASH_REPRODUCER_DIRECTORY` to enable.\n"
     ]
    }
   ],
   "source": [
    "# Convert to TFLite\n",
    "converter = tf.lite.TFLiteConverter.from_keras_model(model)\n",
    "# converter.optimizations = [tf.lite.Optimize.DEFAULT]\n",
    "tflite_model = converter.convert()\n",
    "\n",
    "# Save the model\n",
    "with open('drowsiness_model.tflite', 'wb') as f:\n",
    "    f.write(tflite_model)\n",
    "\n",
    "print(\"Model saved as drowsiness_model.tflite\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.8"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
